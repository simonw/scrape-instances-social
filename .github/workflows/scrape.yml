name: Scrape latest data

on:
  push:
  workflow_dispatch:
    inputs:
      from_scratch:
        description: 'Rebuild database from scratch'
        required: false
        type: boolean
  schedule:
    - cron:  '9,29,40 * * * *'

concurrency: scraper

jobs:
  scheduled:
    runs-on: ubuntu-latest
    steps:
    - name: Check out this repo
      uses: actions/checkout@v3
      with:
        fetch-depth: 0
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.11"
        cache: pip
    - name: Install dependencies
      run: pip install -r requirements.txt
    - name: Fetch latest data
      run: |-
        curl https://instances.social/instances.json | \
          jq 'map(del(.uptime))' > instances.json
    - name: Commit and push if it changed
      run: |-
        git config user.name "Automated"
        git config user.email "actions@users.noreply.github.com"
        git add -A
        timestamp=$(date -u)
        git commit -m "Latest data: ${timestamp}" || exit 0
        git push
    - name: Download previous database
      if: github.event.inputs.from_scratch != 'true'
      run: |
        curl -O https://scrape-instances-social.s3.amazonaws.com/counts.db
    - name: Build and publish database using git-history
      env:
        AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
        AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
      run: |-
        ./build-count-history.sh
        sqlite-utils tables counts.db --counts
        # Upload to S3
        s3-credentials put-object scrape-instances-social counts.db counts.db \
          --access-key $AWS_ACCESS_KEY_ID \
          --secret-key $AWS_SECRET_ACCESS_KEY
